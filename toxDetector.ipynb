{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17008d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toxicity detection model - want -> a contextual languge model akin BERT/RoBERTa/XLnet\n",
    "#want -> \n",
    "# - detection for obvious slangs\n",
    "# - not associate repeated puncutation marks with negative scoring\n",
    "# - ability to provide as possible accurate scoring for sarcastic comments\n",
    "# - more..? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43be67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#core data handling\n",
    "import os\n",
    "import json\n",
    "from typing import List, Optional, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset #rtp streaming\n",
    "\n",
    "#models + evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "import secrets\n",
    "RANDOM_SEED = secrets.randbelow(10000000)\n",
    "#print(\"Random seed:\", RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c9f32f",
   "metadata": {},
   "source": [
    "Helpers for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "690f33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "# - if text/comment field is NaN (empty) -> \"\"\n",
    "# - \" hello world!  \" -> \"hello world\"\n",
    "# - o/w convert to string + remove spaces\n",
    "def cleanText(s: Any) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    if isinstance(s, str):   \n",
    "        return s.strip() \n",
    "    return str(s).strip() \n",
    "\n",
    "#linear scaling: \n",
    "# - clip value (if needed) to lie within the [prev_min, prev_max] range -> prevent outliers\n",
    "# - normalize/scale the value now in [0, 1]  \n",
    "# - stretch to [1, 5] from prev range!\n",
    "def toToxicRange(value: float, prev_min: float, prev_max: float) -> float:\n",
    "    if np.isnan(value):\n",
    "        return np.nan\n",
    "    clipped = float(np.clip(value, prev_min, prev_max))\n",
    "    scaled = (clipped - prev_min) / (prev_max - prev_min)\n",
    "    return 1 + 4 * scaled\n",
    "\n",
    "#Removes only:\n",
    "# - empty strings\n",
    "# - missing values\n",
    "# - '[deleted]', '[removed]'\n",
    "# - pure punctuation - texts with no letters/numbers (e.g. '!!!', '...', '?!!?')\n",
    "def isInformativeText(s: str) -> bool:\n",
    "    if not s:\n",
    "        return False\n",
    "    t = s.strip().lower()\n",
    "    if t in {\"\", \"[deleted]\", \"[removed]\"}:\n",
    "        return False\n",
    "    #remove strings entirely filled w/ punctuation\n",
    "    if isPunctuationOnly(t):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def isPunctuationOnly(s: str) -> bool:\n",
    "    t = s.strip()  #removes whitespace\n",
    "    if len(t) == 0:\n",
    "        return True\n",
    "    \n",
    "    #allowed punctuation chars\n",
    "    punct_chars = set(\"!?,.;:-—'\\\"()[]{}*/\\\\\")\n",
    "    return all(ch in punct_chars for ch in t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1604c6c",
   "metadata": {},
   "source": [
    "Jigsaw unintended bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ffc5ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load jigsaw data from csv and turn multiple binary toxicity columns into a 1–5 score\n",
    "# - reads only `max_rows` rows for speed (if given!)\n",
    "# - returns df with ['text', 'toxicity_score']\n",
    "def loadJigsaw(path: str, text_col: str = \"comment_text\", \n",
    "    tox_cols: Optional[List[str]] = None,\n",
    "    max_rows: Optional[int] = 6000) -> pd.DataFrame:\n",
    "    if tox_cols is None:\n",
    "        tox_cols = [\n",
    "            \"toxic\",\n",
    "            \"severe_toxic\",\n",
    "            \"obscene\",\n",
    "            \"threat\",\n",
    "            \"insult\",\n",
    "            \"identity_hate\",\n",
    "        ]\n",
    "\n",
    "    df = pd.read_csv(path, nrows=max_rows)\n",
    "    df[\"text\"] = df[text_col].apply(cleanText)\n",
    "\n",
    "    #flgs error when required column non-existent in dataset (unlikely)\n",
    "    existing = [c for c in tox_cols if c in df.columns]\n",
    "    if not existing:\n",
    "        raise ValueError(\n",
    "            f\"This column is not present! Here's what we DO have -> {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    for c in existing:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    sum_max = len(existing)  #max possible sum if all are 1\n",
    "    df[\"raw_toxic_sum\"] = df[existing].sum(axis=1)\n",
    "\n",
    "    #maps range [0..sum_max] -> [1..5]\n",
    "    df[\"toxicity_score\"] = df[\"raw_toxic_sum\"].apply(\n",
    "        lambda v: toToxicRange(v, 0.0, float(sum_max))\n",
    "    )\n",
    "\n",
    "    return df[[\"text\", \"toxicity_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76b67c0",
   "metadata": {},
   "source": [
    "Reddit Annotated (Ruddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51924f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads ruddit csv with each row score in [-1, 1] -> converted \n",
    "# to a [1–5] toxicity scale\n",
    "def loadRuddit(path: str, text_col: str = \"body\",\n",
    "    score_col: str = \"score\",\n",
    "    #we're using the whole (short) dataset\n",
    "    max_rows: Optional[int] = None) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, nrows=max_rows)\n",
    "    df[\"text\"] = df[text_col].apply(cleanText)\n",
    "    df[score_col] = pd.to_numeric(df[score_col], errors=\"coerce\")\n",
    "\n",
    "    #entries with missing score removed!\n",
    "    df = df.dropna(subset=[score_col])\n",
    "\n",
    "    df[\"toxicity_score\"] = df[score_col].apply(\n",
    "        lambda v: toToxicRange(v, -1.0, 1.0)  #from [-1, 1] -> [1, 5]\n",
    "    )\n",
    "\n",
    "    return df[[\"text\", \"toxicity_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de2089",
   "metadata": {},
   "source": [
    "Real Toxcicity Prompts (allenai) - processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5dc2da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#streaming sample of RTP data from huggingface \n",
    "# - convert to [1–5] scale\n",
    "# - uses 'continuation.toxicity' if available, else 'prompt.toxicity\n",
    "# - toxicity scores are originally in [0,1]\n",
    "def loadRTP(n_samples=6000, prefer_continuation=True):\n",
    "    stream = load_dataset(\n",
    "        \"allenai/real-toxicity-prompts\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i, rec in enumerate(stream):\n",
    "        if i >= n_samples:\n",
    "            break\n",
    "        \n",
    "        p = rec.get(\"prompt\", {})\n",
    "        c = rec.get(\"continuation\", {})\n",
    "\n",
    "        prompt_text = cleanText(p.get(\"text\", \"\")) if isinstance(p, dict) else \"\"\n",
    "        cont_text = cleanText(c.get(\"text\", \"\")) if isinstance(c, dict) else \"\"\n",
    "\n",
    "        if prefer_continuation and cont_text:\n",
    "            text = cont_text\n",
    "            tox = c.get(\"toxicity\", p.get(\"toxicity\", None))\n",
    "        else:\n",
    "            text = (prompt_text + \" \" + cont_text).strip()\n",
    "            tox = c.get(\"toxicity\", p.get(\"toxicity\", None))\n",
    "\n",
    "        rows.append({\"text\": text, \"raw_toxicity\": tox})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"raw_toxicity\"] = pd.to_numeric(df[\"raw_toxicity\"], errors=\"coerce\")\n",
    "    df[\"toxicity_score\"] = df[\"raw_toxicity\"].apply(\n",
    "        lambda v: toToxicRange(v, 0.0, 1.0)\n",
    "    )\n",
    "\n",
    "    return df[[\"text\", \"toxicity_score\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6344ac0d",
   "metadata": {},
   "source": [
    "clean and merge helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bb9b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applies a 2nd round of cleaning:\n",
    "# - enforces text type,\n",
    "# - removes empty / '[deleted]' / very short - if any\n",
    "# - drop duplicates\n",
    "def cleanAndMergeTrainingData(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].apply(cleanText)\n",
    "    df = df[df[\"text\"].apply(isInformativeText)]\n",
    "\n",
    "    df = df.dropna(subset=[\"toxicity_score\"])\n",
    "    df = df[df[\"toxicity_score\"].apply(lambda x: not pd.isna(x))]\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"text\"])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "#merge multiple [text, toxicity_score] dfs into one\n",
    "def mergeDatasets(dfs: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    cleaned = []\n",
    "    for d in dfs:\n",
    "        d = d.copy()\n",
    "        if not {\"text\", \"toxicity_score\"}.issubset(d.columns):\n",
    "            raise ValueError(\"Dataset missing required columns.\")\n",
    "        cleaned.append(d[[\"text\", \"toxicity_score\"]])\n",
    "\n",
    "    combined = pd.concat(cleaned, ignore_index=True)\n",
    "    combined = cleanAndMergeTrainingData(combined)\n",
    "    combined = combined.sample(frac=1.0, random_state=42)\n",
    "\n",
    "    return combined.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62f42dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 2)\n",
      "(5966, 2)\n",
      "(6000, 2)\n",
      "Combined dataframe looks like this ->  (17667, 2)\n",
      "                                                text  toxicity_score\n",
      "0  It's not blind if we are following other count...        2.666000\n",
      "1  week earlier in a 47-34 loss at Mississippi St...        1.309592\n",
      "2        Unit Test Project and a sample MVC project.        1.022699\n",
      "3  I hate spiders, but I hate mosquitos, wasps an...        3.458000\n",
      "4  I usually excuse myself to the bathroom and wa...        3.852000\n"
     ]
    }
   ],
   "source": [
    "jigsawDF = loadJigsaw(\"jigsaw_train.csv\")\n",
    "rudditDF = loadRuddit(\"ruddit_train.csv\")\n",
    "rtpDF = loadRTP(6000)\n",
    "\n",
    "#individual dataframes\n",
    "print(jigsawDF.shape) #prints -> (# of entries, # of columns) \n",
    "print(rudditDF.shape)\n",
    "print(rtpDF.shape)\n",
    "\n",
    "#combined dataframe\n",
    "combinedDF = mergeDatasets([jigsawDF, rudditDF, rtpDF])\n",
    "print(\"Combined dataframe looks like this -> \", combinedDF.shape)\n",
    "print(combinedDF.head())\n",
    "\n",
    "combinedDF.to_csv(\"combined_toxicity.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
