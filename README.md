# README: Project Info Dump!
This project explores how sentiment analysis models classify online comments as “toxic” or “non-toxic,” and questions how fair or accurate those classifications really are. Our goal is to take a human-centered approach to AI by examining how algorithms interpret humor, sarcasm, and emotional expression—especially when such content is mislabeled as toxic.

The project will also include a visualization component to display findings from our analysis.
